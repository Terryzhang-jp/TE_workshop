"""
Calculate SHAP Data for Visualizations
è®¡ç®—SHAPå¯è§†åŒ–æ•°æ®å¹¶è¾“å‡ºä¸ºJSONå’ŒCSVæ ¼å¼
"""

import pandas as pd
import numpy as np
import json
import shap
import xgboost as xgb
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
import lime
import lime.lime_tabular
import warnings
warnings.filterwarnings('ignore')

class SHAPDataCalculator:
    """SHAPæ•°æ®è®¡ç®—å™¨"""
    
    def __init__(self, data_path):
        self.data_path = data_path
        self.model = None
        self.scaler = None
        self.explainer = None
        self.shap_values = None
        self.lime_explainer = None
        self.feature_columns = ['temp', 'hour', 'day_of_week', 'week_of_month']
        self.feature_names = ['Temperature', 'Hour', 'Day_of_Week', 'Week_of_Month']
        
    def load_and_prepare_data(self):
        """åŠ è½½å’Œå‡†å¤‡æ•°æ®"""
        print("ðŸ“Š åŠ è½½æ•°æ®...")
        
        # åŠ è½½æ•°æ®
        df = pd.read_csv(self.data_path)
        df['æ—¶é—´'] = pd.to_datetime(df['æ—¶é—´'])
        df = df.rename(columns={
            'æ—¶é—´': 'time', 'é¢„æµ‹ç”µåŠ›': 'predicted_power',
            'çœŸå®žç”µåŠ›': 'actual_power', 'é¢„æµ‹å¤©æ°”': 'predicted_temp'
        })
        
        # ç‰¹å¾å·¥ç¨‹
        df['hour'] = df['time'].dt.hour
        df['day_of_week'] = df['time'].dt.dayofweek
        df['week_of_month'] = (df['time'].dt.day - 1) // 7 + 1
        df['temp'] = df['predicted_temp']
        
        # åˆ†ç¦»æ•°æ®
        self.train_data = df[df['actual_power'].notna()].copy()
        self.predict_data = df[df['actual_power'].isna()].copy()
        
        print(f"âœ… è®­ç»ƒæ•°æ®: {len(self.train_data)} æ¡")
        print(f"âœ… é¢„æµ‹æ•°æ®: {len(self.predict_data)} æ¡")
        
        return df
        
    def train_model(self):
        """è®­ç»ƒæ¨¡åž‹"""
        print("ðŸ¤– è®­ç»ƒXGBoostæ¨¡åž‹...")
        
        X_train = self.train_data[self.feature_columns].values
        y_train = self.train_data['actual_power'].values
        
        self.scaler = StandardScaler()
        X_train_scaled = self.scaler.fit_transform(X_train)
        
        self.model = xgb.XGBRegressor(
            n_estimators=100, max_depth=6, learning_rate=0.1,
            random_state=42, objective='reg:squarederror'
        )
        self.model.fit(X_train_scaled, y_train)
        
        # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
        y_pred = self.model.predict(X_train_scaled)
        mae = mean_absolute_error(y_train, y_pred)
        rmse = np.sqrt(mean_squared_error(y_train, y_pred))
        
        print(f"âœ… æ¨¡åž‹è®­ç»ƒå®Œæˆ - MAE: {mae:.2f} MW, RMSE: {rmse:.2f} MW")
        
    def initialize_shap(self):
        """åˆå§‹åŒ–SHAP"""
        print("ðŸ” åˆå§‹åŒ–SHAPåˆ†æžå™¨...")
        
        X_train_scaled = self.scaler.transform(self.train_data[self.feature_columns].values)
        self.explainer = shap.TreeExplainer(self.model, data=X_train_scaled)
        
        X_predict_scaled = self.scaler.transform(self.predict_data[self.feature_columns].values)
        self.shap_values = self.explainer.shap_values(X_predict_scaled)
        
        print("âœ… SHAPåˆ†æžå™¨åˆå§‹åŒ–å®Œæˆ")

    def initialize_lime(self):
        """åˆå§‹åŒ–LIMEåˆ†æžå™¨"""
        print("ðŸ” åˆå§‹åŒ–LIMEåˆ†æžå™¨...")

        X_train_scaled = self.scaler.transform(self.train_data[self.feature_columns].values)

        # åˆ›å»ºLIMEè§£é‡Šå™¨
        self.lime_explainer = lime.lime_tabular.LimeTabularExplainer(
            X_train_scaled,
            feature_names=self.feature_names,
            class_names=['Power_Consumption'],
            mode='regression',
            discretize_continuous=False,  # ä¸ç¦»æ•£åŒ–è¿žç»­ç‰¹å¾
            random_state=42
        )

        print("âœ… LIMEåˆ†æžå™¨åˆå§‹åŒ–å®Œæˆ")
        
    def calculate_feature_importance_data(self):
        """è®¡ç®—ç‰¹å¾é‡è¦æ€§æ•°æ®"""
        print("ðŸ“Š è®¡ç®—ç‰¹å¾é‡è¦æ€§æ•°æ®...")
        
        # è®¡ç®—å¹³å‡ç»å¯¹SHAPå€¼
        mean_abs_shap = np.mean(np.abs(self.shap_values), axis=0)
        
        # åˆ›å»ºæ•°æ®
        importance_data = []
        for i, (feature, importance) in enumerate(zip(self.feature_names, mean_abs_shap)):
            importance_data.append({
                'feature': feature,
                'feature_chinese': ['æ¸©åº¦', 'å°æ—¶', 'æ˜ŸæœŸ', 'å‘¨æ•°'][i],
                'importance': float(importance),
                'rank': int(np.argsort(mean_abs_shap)[::-1].tolist().index(i) + 1)
            })
        
        # æŒ‰é‡è¦æ€§æŽ’åº
        importance_data.sort(key=lambda x: x['importance'], reverse=True)
        
        return importance_data
        
    def calculate_dependence_data(self):
        """ä½¿ç”¨è®­ç»ƒæ•°æ®è®¡ç®—çœŸæ­£çš„ç‰¹å¾ä¾èµ–æ•°æ®"""
        print("ðŸ“ˆ è®¡ç®—ç‰¹å¾ä¾èµ–æ•°æ®...")

        # å¯¹è®­ç»ƒæ•°æ®è®¡ç®—SHAPå€¼
        X_train_scaled = self.scaler.transform(self.train_data[self.feature_columns].values)
        train_shap_values = self.explainer.shap_values(X_train_scaled)

        dependence_data = {}

        # ä¸ºæ¯ä¸ªç‰¹å¾è®¡ç®—dependenceæ•°æ®
        for feature_idx, (feature_name, feature_chinese) in enumerate(zip(self.feature_names, ['æ¸©åº¦', 'å°æ—¶', 'æ˜ŸæœŸ', 'å‘¨æ•°'])):
            print(f"   è®¡ç®— {feature_chinese} çš„ä¾èµ–æ•°æ®...")

            # æå–è¯¥ç‰¹å¾çš„å€¼å’Œå¯¹åº”çš„SHAPå€¼
            feature_values = self.train_data[self.feature_columns[feature_idx]].values
            shap_values = train_shap_values[:, feature_idx]

            # åˆ›å»ºæ•°æ®ç‚¹
            feature_data = []
            for i, (feat_val, shap_val) in enumerate(zip(feature_values, shap_values)):
                feature_data.append({
                    'feature_value': float(feat_val),
                    'shap_value': float(shap_val),
                    'sample_index': i
                })

            # æŒ‰ç‰¹å¾å€¼æŽ’åº
            feature_data.sort(key=lambda x: x['feature_value'])

            dependence_data[feature_name] = {
                'feature': feature_name,
                'feature_chinese': feature_chinese,
                'data_points': feature_data,
                'value_range': {
                    'min': float(np.min(feature_values)),
                    'max': float(np.max(feature_values))
                },
                'shap_range': {
                    'min': float(np.min(shap_values)),
                    'max': float(np.max(shap_values))
                },
                'total_points': len(feature_data),
                'unique_values': len(np.unique(feature_values))
            }

        return dependence_data

    def calculate_lime_data(self):
        """è®¡ç®—æ¯ä¸ªå°æ—¶çš„LIMEè§£é‡Š"""
        print("ðŸ‹ è®¡ç®—LIMEè§£é‡Šæ•°æ®...")

        lime_data = {
            'hourly_explanations': [],
            'feature_importance_by_hour': {},
            'summary': {
                'total_hours': len(self.predict_data),
                'features': self.feature_names,
                'features_chinese': ['æ¸©åº¦', 'å°æ—¶', 'æ˜ŸæœŸ', 'å‘¨æ•°']
            }
        }

        # ä¸ºæ¯ä¸ªé¢„æµ‹å°æ—¶ç”ŸæˆLIMEè§£é‡Š
        for i, row in self.predict_data.iterrows():
            hour_idx = i - self.predict_data.index[0]
            hour = int(row['hour'])

            print(f"   è®¡ç®— {hour}:00 çš„LIMEè§£é‡Š...")

            # å‡†å¤‡å®žä¾‹æ•°æ®
            instance = self.predict_data[self.feature_columns].iloc[hour_idx].values
            instance_scaled = self.scaler.transform([instance])[0]

            # èŽ·å–æ¨¡åž‹é¢„æµ‹
            prediction = self.model.predict([instance_scaled])[0]

            # ç”ŸæˆLIMEè§£é‡Š
            explanation = self.lime_explainer.explain_instance(
                instance_scaled,
                self.model.predict,
                num_features=len(self.feature_names),
                num_samples=1000
            )

            # æå–ç‰¹å¾è´¡çŒ®
            feature_contributions = {}
            explanation_list = explanation.as_list()
            explanation_map = explanation.as_map()

            # èŽ·å–å±€éƒ¨é¢„æµ‹å€¼å’Œæˆªè·
            local_pred = explanation.local_pred[0] if hasattr(explanation, 'local_pred') else prediction
            intercept = explanation.intercept[0] if hasattr(explanation, 'intercept') else 0

            for feature_idx, (feature_name, feature_chinese) in enumerate(zip(self.feature_names, ['æ¸©åº¦', 'å°æ—¶', 'æ˜ŸæœŸ', 'å‘¨æ•°'])):
                # ä»ŽLIMEè§£é‡Šä¸­èŽ·å–è¯¥ç‰¹å¾çš„è´¡çŒ®
                lime_contribution = 0

                # æ–¹æ³•1: ä½¿ç”¨as_map()èŽ·å–ç‰¹å¾è´¡çŒ®
                if 1 in explanation_map:  # å›žå½’ä»»åŠ¡é€šå¸¸ä½¿ç”¨ç±»åˆ«1
                    for feat_idx, contribution in explanation_map[1]:
                        if feat_idx == feature_idx:
                            lime_contribution = contribution
                            break

                # æ–¹æ³•2: å¦‚æžœas_map()æ²¡æœ‰ç»“æžœï¼Œä½¿ç”¨as_list()
                if lime_contribution == 0:
                    for feature_desc, contribution in explanation_list:
                        if feature_name in feature_desc or f"feature_{feature_idx}" in feature_desc:
                            lime_contribution = contribution
                            break

                feature_contributions[feature_name] = {
                    'contribution': float(lime_contribution),
                    'feature_value': float(instance[feature_idx]),
                    'feature_chinese': feature_chinese
                }

            # æŒ‰è´¡çŒ®åº¦æŽ’åº
            sorted_contributions = sorted(
                feature_contributions.items(),
                key=lambda x: abs(x[1]['contribution']),
                reverse=True
            )

            hour_explanation = {
                'hour': hour,
                'time': row['time'].strftime('%H:%M'),
                'prediction': float(prediction),
                'feature_contributions': feature_contributions,
                'sorted_contributions': [
                    {
                        'feature': item[0],
                        'feature_chinese': item[1]['feature_chinese'],
                        'contribution': item[1]['contribution'],
                        'feature_value': item[1]['feature_value'],
                        'abs_contribution': abs(item[1]['contribution'])
                    }
                    for item in sorted_contributions
                ],
                'explanation_text': self._generate_lime_explanation_text(sorted_contributions, hour)
            }

            lime_data['hourly_explanations'].append(hour_explanation)

        # è®¡ç®—æ¯ä¸ªç‰¹å¾åœ¨ä¸åŒå°æ—¶çš„é‡è¦æ€§å˜åŒ–
        for feature_name in self.feature_names:
            lime_data['feature_importance_by_hour'][feature_name] = [
                exp['feature_contributions'][feature_name]['contribution']
                for exp in lime_data['hourly_explanations']
            ]

        return lime_data

    def _generate_lime_explanation_text(self, sorted_contributions, hour):
        """ç”ŸæˆLIMEè§£é‡Šæ–‡æœ¬"""
        if not sorted_contributions:
            return f"{hour}:00 - æ— æ³•ç”Ÿæˆè§£é‡Š"

        top_positive = None
        top_negative = None

        for feature, data in sorted_contributions:
            if data['contribution'] > 0 and top_positive is None:
                top_positive = (feature, data)
            elif data['contribution'] < 0 and top_negative is None:
                top_negative = (feature, data)

            if top_positive and top_negative:
                break

        explanation_parts = [f"{hour}:00æ—¶åˆ»"]

        if top_positive:
            explanation_parts.append(
                f"{top_positive[1]['feature_chinese']}æœ€å¤§ç¨‹åº¦å¢žåŠ ç”¨ç”µéœ€æ±‚(+{top_positive[1]['contribution']:.1f})"
            )

        if top_negative:
            explanation_parts.append(
                f"{top_negative[1]['feature_chinese']}æœ€å¤§ç¨‹åº¦å‡å°‘ç”¨ç”µéœ€æ±‚({top_negative[1]['contribution']:.1f})"
            )

        return "ï¼Œ".join(explanation_parts)
        
    def calculate_all_data(self):
        """è®¡ç®—SHAPå’ŒLIMEæ•°æ®"""
        print("ðŸš€ å¼€å§‹è®¡ç®—SHAPå’ŒLIMEæ•°æ®...")

        # å‡†å¤‡æ•°æ®å’Œæ¨¡åž‹
        self.load_and_prepare_data()
        self.train_model()
        self.initialize_shap()
        self.initialize_lime()

        # è®¡ç®—æ ¸å¿ƒæ•°æ®
        all_data = {
            'metadata': {
                'date': '2022-01-07',
                'description': 'SHAP and LIME analysis for January 7th power prediction',
                'features': self.feature_names,
                'features_chinese': ['æ¸©åº¦', 'å°æ—¶', 'æ˜ŸæœŸ', 'å‘¨æ•°'],
                'base_prediction': float(self.explainer.expected_value),
                'total_hours': len(self.predict_data)
            },
            'shap_analysis': {
                'feature_importance': self.calculate_feature_importance_data(),
                'feature_dependence': self.calculate_dependence_data()
            },
            'lime_analysis': self.calculate_lime_data()
        }

        return all_data
        
    def save_data(self, data, output_dir='/Users/yichuanzhang/Desktop/workshop_TE/backend/'):
        """ä¿å­˜æ•°æ®ä¸ºJSONå’ŒCSVæ ¼å¼"""
        print("ðŸ’¾ ä¿å­˜æ•°æ®...")

        # ä¿å­˜å®Œæ•´JSON
        json_path = f"{output_dir}shap_data_complete.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… å®Œæ•´JSONæ•°æ®å·²ä¿å­˜: {json_path}")

        # ä¿å­˜å„éƒ¨åˆ†CSV
        # 1. SHAPç‰¹å¾é‡è¦æ€§
        importance_df = pd.DataFrame(data['shap_analysis']['feature_importance'])
        importance_csv = f"{output_dir}shap_feature_importance.csv"
        importance_df.to_csv(importance_csv, index=False, encoding='utf-8')
        print(f"âœ… SHAPç‰¹å¾é‡è¦æ€§CSVå·²ä¿å­˜: {importance_csv}")

        # 2. SHAPç‰¹å¾ä¾èµ–æ•°æ® - ä¸ºæ¯ä¸ªç‰¹å¾ä¿å­˜å•ç‹¬çš„CSV
        dependence_csvs = {}
        for feature_name, feature_data in data['shap_analysis']['feature_dependence'].items():
            # å±•å¼€æ•°æ®ç‚¹
            dependence_df = pd.DataFrame(feature_data['data_points'])
            dependence_df['feature_name'] = feature_name
            dependence_df['feature_chinese'] = feature_data['feature_chinese']

            dependence_csv = f"{output_dir}shap_dependence_{feature_name.lower()}.csv"
            dependence_df.to_csv(dependence_csv, index=False, encoding='utf-8')
            dependence_csvs[feature_name] = dependence_csv
            print(f"âœ… SHAP {feature_data['feature_chinese']}ä¾èµ–CSVå·²ä¿å­˜: {dependence_csv}")

        # 3. LIMEå°æ—¶è§£é‡Šæ•°æ®
        lime_hourly_df = pd.DataFrame(data['lime_analysis']['hourly_explanations'])
        lime_hourly_csv = f"{output_dir}lime_hourly_explanations.csv"
        lime_hourly_df.to_csv(lime_hourly_csv, index=False, encoding='utf-8')
        print(f"âœ… LIMEå°æ—¶è§£é‡ŠCSVå·²ä¿å­˜: {lime_hourly_csv}")

        # 4. LIMEç‰¹å¾é‡è¦æ€§æ—¶é—´åºåˆ—
        lime_importance_data = []
        for hour_data in data['lime_analysis']['hourly_explanations']:
            for feature_name, contrib_data in hour_data['feature_contributions'].items():
                lime_importance_data.append({
                    'hour': hour_data['hour'],
                    'time': hour_data['time'],
                    'feature': feature_name,
                    'feature_chinese': contrib_data['feature_chinese'],
                    'contribution': contrib_data['contribution'],
                    'feature_value': contrib_data['feature_value']
                })

        lime_importance_df = pd.DataFrame(lime_importance_data)
        lime_importance_csv = f"{output_dir}lime_feature_importance_by_hour.csv"
        lime_importance_df.to_csv(lime_importance_csv, index=False, encoding='utf-8')
        print(f"âœ… LIMEç‰¹å¾é‡è¦æ€§æ—¶é—´åºåˆ—CSVå·²ä¿å­˜: {lime_importance_csv}")

        return {
            'json': json_path,
            'csvs': {
                'shap_feature_importance': importance_csv,
                'shap_dependence': dependence_csvs,
                'lime_hourly_explanations': lime_hourly_csv,
                'lime_feature_importance_by_hour': lime_importance_csv
            }
        }

def main():
    """ä¸»å‡½æ•°"""
    print("ðŸŽ¯ SHAPæ•°æ®è®¡ç®—å™¨")
    print("=" * 50)
    
    # åˆå§‹åŒ–è®¡ç®—å™¨
    data_path = "/Users/yichuanzhang/Desktop/workshop_TE/backend/data/worst_day_1_2022_01_07_winter_extreme_cold.csv"
    calculator = SHAPDataCalculator(data_path)
    
    # è®¡ç®—æ‰€æœ‰æ•°æ®
    all_data = calculator.calculate_all_data()
    
    # ä¿å­˜æ•°æ®
    file_paths = calculator.save_data(all_data)
    
    print("\nðŸŽ‰ SHAPå’ŒLIMEæ•°æ®è®¡ç®—å®Œæˆï¼")
    print("ðŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"   ðŸ“„ å®Œæ•´JSON: {file_paths['json']}")
    print("   ðŸ“Š CSVæ–‡ä»¶:")
    print(f"      â€¢ SHAPç‰¹å¾é‡è¦æ€§: {file_paths['csvs']['shap_feature_importance']}")
    for feature_name, path in file_paths['csvs']['shap_dependence'].items():
        print(f"      â€¢ SHAP {feature_name}ä¾èµ–: {path}")
    print(f"      â€¢ LIMEå°æ—¶è§£é‡Š: {file_paths['csvs']['lime_hourly_explanations']}")
    print(f"      â€¢ LIMEç‰¹å¾é‡è¦æ€§æ—¶é—´åºåˆ—: {file_paths['csvs']['lime_feature_importance_by_hour']}")

    # æ˜¾ç¤ºæ•°æ®æ‘˜è¦
    print(f"\nðŸ“ˆ æ•°æ®æ‘˜è¦:")
    print(f"   ðŸ” SHAPåˆ†æž:")
    print(f"     â€¢ ç‰¹å¾é‡è¦æ€§: {len(all_data['shap_analysis']['feature_importance'])} ä¸ªç‰¹å¾")
    print(f"     â€¢ ç‰¹å¾ä¾èµ–: {len(all_data['shap_analysis']['feature_dependence'])} ä¸ªç‰¹å¾")
    for feature_name, feature_data in all_data['shap_analysis']['feature_dependence'].items():
        print(f"       - {feature_data['feature_chinese']}: {feature_data['total_points']} ä¸ªæ•°æ®ç‚¹")

    print(f"   ðŸ‹ LIMEåˆ†æž:")
    print(f"     â€¢ å°æ—¶è§£é‡Š: {len(all_data['lime_analysis']['hourly_explanations'])} ä¸ªå°æ—¶")
    print(f"     â€¢ æ¯å°æ—¶ç‰¹å¾è´¡çŒ®: {len(all_data['lime_analysis']['feature_importance_by_hour'])} ä¸ªç‰¹å¾")

if __name__ == "__main__":
    main()
