#!/usr/bin/env python3
"""
æ·±åº¦éªŒè¯SHAPè®¡ç®—çš„åˆç†æ€§
æ£€æŸ¥SHAPå€¼çš„æ•°å­¦æ€§è´¨ã€ä¸€è‡´æ€§å’Œè§£é‡Šæ€§
"""

import pandas as pd
import numpy as np
import shap
import xgboost as xgb
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

class SHAPValidationAnalyzer:
    def __init__(self, data_path='data/worst_day_1_2022_01_07_winter_extreme_cold.csv'):
        self.data_path = data_path
        self.model = None
        self.scaler = None
        self.explainer = None
        self.shap_values = None
        self.feature_columns = ['temp', 'hour', 'day_of_week', 'week_of_month']
        self.feature_names = ['Temperature', 'Hour', 'Day_of_Week', 'Week_of_Month']
        
    def load_and_prepare_data(self):
        """åŠ è½½å’Œå‡†å¤‡æ•°æ®"""
        print("ğŸ“‚ åŠ è½½å’Œå‡†å¤‡æ•°æ®...")
        
        # åŠ è½½æ•°æ®
        df = pd.read_csv(self.data_path)
        df['æ—¶é—´'] = pd.to_datetime(df['æ—¶é—´'])
        df = df.rename(columns={
            'æ—¶é—´': 'time', 'é¢„æµ‹ç”µåŠ›': 'predicted_power',
            'çœŸå®ç”µåŠ›': 'actual_power', 'é¢„æµ‹å¤©æ°”': 'predicted_temp'
        })
        
        # ç‰¹å¾å·¥ç¨‹
        df['hour'] = df['time'].dt.hour
        df['day_of_week'] = df['time'].dt.dayofweek
        df['week_of_month'] = (df['time'].dt.day - 1) // 7 + 1
        df['temp'] = df['predicted_temp']
        
        # åˆ†ç¦»æ•°æ®
        self.train_data = df[df['actual_power'].notna()].copy()
        self.predict_data = df[df['predicted_power'].notna()].copy()
        
        print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ")
        print(f"   è®­ç»ƒæ•°æ®: {self.train_data.shape}")
        print(f"   é¢„æµ‹æ•°æ®: {self.predict_data.shape}")
        
    def train_and_validate_model(self):
        """è®­ç»ƒæ¨¡å‹å¹¶éªŒè¯æ€§èƒ½"""
        print("ğŸ¤– è®­ç»ƒå¹¶éªŒè¯æ¨¡å‹...")
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        X_train = self.train_data[self.feature_columns].values
        y_train = self.train_data['actual_power'].values
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        self.scaler = StandardScaler()
        X_train_scaled = self.scaler.fit_transform(X_train)
        
        # è®­ç»ƒXGBoostæ¨¡å‹
        self.model = xgb.XGBRegressor(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=42
        )
        self.model.fit(X_train_scaled, y_train)
        
        # éªŒè¯æ¨¡å‹æ€§èƒ½
        y_pred = self.model.predict(X_train_scaled)
        mae = mean_absolute_error(y_train, y_pred)
        rmse = np.sqrt(mean_squared_error(y_train, y_pred))
        r2 = r2_score(y_train, y_pred)
        
        print(f"   æ¨¡å‹æ€§èƒ½: MAE={mae:.2f}, RMSE={rmse:.2f}, RÂ²={r2:.3f}")
        
        return {'mae': mae, 'rmse': rmse, 'r2': r2}
        
    def initialize_and_calculate_shap(self):
        """åˆå§‹åŒ–SHAPå¹¶è®¡ç®—å€¼"""
        print("ğŸ” åˆå§‹åŒ–SHAPå¹¶è®¡ç®—å€¼...")
        
        # å‡†å¤‡æ•°æ®
        X_train_scaled = self.scaler.transform(self.train_data[self.feature_columns].values)
        X_predict_scaled = self.scaler.transform(self.predict_data[self.feature_columns].values)
        
        # åˆå§‹åŒ–SHAPè§£é‡Šå™¨
        self.explainer = shap.TreeExplainer(self.model, data=X_train_scaled)
        
        # è®¡ç®—SHAPå€¼
        self.shap_values = self.explainer.shap_values(X_predict_scaled)
        
        print(f"âœ… SHAPè®¡ç®—å®Œæˆ")
        print(f"   SHAPå€¼å½¢çŠ¶: {self.shap_values.shape}")
        print(f"   åŸºå‡†å€¼: {self.explainer.expected_value:.2f}")
        
    def validate_shap_properties(self):
        """éªŒè¯SHAPçš„æ•°å­¦æ€§è´¨"""
        print("\nğŸ§® éªŒè¯SHAPæ•°å­¦æ€§è´¨...")
        
        # å‡†å¤‡æ•°æ®
        X_predict_scaled = self.scaler.transform(self.predict_data[self.feature_columns].values)
        predictions = self.model.predict(X_predict_scaled)
        
        # 1. æ•ˆç‡æ€§è´¨éªŒè¯ (Efficiency): sum(SHAP) + base_value = prediction
        print("\n1. æ•ˆç‡æ€§è´¨éªŒè¯ (Efficiency):")
        shap_sums = np.sum(self.shap_values, axis=1)
        expected_predictions = shap_sums + self.explainer.expected_value
        efficiency_errors = np.abs(predictions - expected_predictions)
        
        print(f"   å¹³å‡æ•ˆç‡è¯¯å·®: {np.mean(efficiency_errors):.6f}")
        print(f"   æœ€å¤§æ•ˆç‡è¯¯å·®: {np.max(efficiency_errors):.6f}")
        print(f"   æ•ˆç‡æ€§è´¨æ»¡è¶³: {'âœ…' if np.max(efficiency_errors) < 1e-6 else 'âŒ'}")
        
        # 2. å¯¹ç§°æ€§éªŒè¯ (Symmetry): ç›¸åŒè´¡çŒ®çš„ç‰¹å¾åº”æœ‰ç›¸åŒSHAPå€¼
        print("\n2. å¯¹ç§°æ€§éªŒè¯:")
        # æ£€æŸ¥ç›¸åŒç‰¹å¾å€¼çš„SHAPå€¼åˆ†å¸ƒ
        for i, feature in enumerate(self.feature_names):
            feature_values = self.predict_data[self.feature_columns[i]].values
            shap_feature = self.shap_values[:, i]
            
            # è®¡ç®—ç›¸åŒç‰¹å¾å€¼çš„SHAPå€¼æ ‡å‡†å·®
            unique_values = np.unique(feature_values)
            if len(unique_values) > 1:
                symmetry_scores = []
                for val in unique_values:
                    mask = feature_values == val
                    if np.sum(mask) > 1:
                        shap_std = np.std(shap_feature[mask])
                        symmetry_scores.append(shap_std)
                
                if symmetry_scores:
                    avg_symmetry = np.mean(symmetry_scores)
                    print(f"   {feature}: å¹³å‡å¯¹ç§°æ€§åˆ†æ•° = {avg_symmetry:.3f}")
        
        # 3. è™šæ‹Ÿæ€§éªŒè¯ (Dummy): ä¸å½±å“é¢„æµ‹çš„ç‰¹å¾SHAPå€¼åº”ä¸º0
        print("\n3. è™šæ‹Ÿæ€§éªŒè¯:")
        # æ£€æŸ¥ç‰¹å¾é‡è¦æ€§ä¸SHAPå€¼çš„ä¸€è‡´æ€§
        feature_importance = self.model.feature_importances_
        shap_importance = np.mean(np.abs(self.shap_values), axis=0)
        
        correlation = np.corrcoef(feature_importance, shap_importance)[0, 1]
        print(f"   XGBoosté‡è¦æ€§ä¸SHAPé‡è¦æ€§ç›¸å…³æ€§: {correlation:.3f}")
        print(f"   ä¸€è‡´æ€§éªŒè¯: {'âœ…' if correlation > 0.8 else 'âŒ'}")
        
        # 4. å¯åŠ æ€§éªŒè¯ (Additivity)
        print("\n4. å¯åŠ æ€§éªŒè¯:")
        # å¯¹äºçº¿æ€§æ¨¡å‹ï¼ŒSHAPå€¼åº”è¯¥ç­‰äºç‰¹å¾å€¼ä¹˜ä»¥æƒé‡
        # å¯¹äºæ ‘æ¨¡å‹ï¼Œæ£€æŸ¥SHAPå€¼çš„åˆç†èŒƒå›´
        for i, feature in enumerate(self.feature_names):
            shap_range = [np.min(self.shap_values[:, i]), np.max(self.shap_values[:, i])]
            feature_range = [np.min(self.predict_data[self.feature_columns[i]]), 
                           np.max(self.predict_data[self.feature_columns[i]])]
            print(f"   {feature}:")
            print(f"      SHAPèŒƒå›´: [{shap_range[0]:.2f}, {shap_range[1]:.2f}]")
            print(f"      ç‰¹å¾èŒƒå›´: [{feature_range[0]:.2f}, {feature_range[1]:.2f}]")
        
        return {
            'efficiency_error': np.mean(efficiency_errors),
            'consistency_correlation': correlation,
            'shap_ranges': {feature: [float(np.min(self.shap_values[:, i])), 
                                    float(np.max(self.shap_values[:, i]))] 
                          for i, feature in enumerate(self.feature_names)}
        }
        
    def validate_business_logic(self):
        """éªŒè¯ä¸šåŠ¡é€»è¾‘çš„åˆç†æ€§"""
        print("\nğŸ’¼ éªŒè¯ä¸šåŠ¡é€»è¾‘åˆç†æ€§...")
        
        business_validation = {}
        
        # 1. å°æ—¶ç‰¹å¾çš„åˆç†æ€§
        print("\n1. å°æ—¶ç‰¹å¾åˆ†æ:")
        hour_shap = {}
        for hour in range(24):
            mask = self.predict_data['hour'] == hour
            if np.sum(mask) > 0:
                avg_shap = np.mean(self.shap_values[mask, 1])  # Houræ˜¯ç¬¬2ä¸ªç‰¹å¾
                hour_shap[hour] = avg_shap
        
        # æ‰¾å‡ºç”¨ç”µé«˜å³°å’Œä½è°·
        peak_hours = sorted(hour_shap.items(), key=lambda x: x[1], reverse=True)[:3]
        low_hours = sorted(hour_shap.items(), key=lambda x: x[1])[:3]
        
        print(f"   ç”¨ç”µé«˜å³°æ—¶æ®µ: {[f'{h}:00({v:.1f})' for h, v in peak_hours]}")
        print(f"   ç”¨ç”µä½è°·æ—¶æ®µ: {[f'{h}:00({v:.1f})' for h, v in low_hours]}")
        
        # éªŒè¯æ˜¯å¦ç¬¦åˆå¸¸è¯†ï¼šæ™šä¸Šå’Œæ—©ä¸Šåº”è¯¥æ˜¯é«˜å³°
        expected_peaks = [7, 8, 9, 18, 19, 20, 21]
        actual_peaks = [h for h, v in peak_hours]
        peak_overlap = len(set(expected_peaks) & set(actual_peaks))
        
        print(f"   é«˜å³°æ—¶æ®µç¬¦åˆé¢„æœŸ: {'âœ…' if peak_overlap >= 1 else 'âŒ'}")
        
        business_validation['hour_logic'] = {
            'peak_hours': peak_hours,
            'low_hours': low_hours,
            'peak_overlap': peak_overlap
        }
        
        # 2. æ¸©åº¦ç‰¹å¾çš„åˆç†æ€§
        print("\n2. æ¸©åº¦ç‰¹å¾åˆ†æ:")
        temp_shap_corr = np.corrcoef(
            self.predict_data['temp'].values,
            self.shap_values[:, 0]  # Temperatureæ˜¯ç¬¬1ä¸ªç‰¹å¾
        )[0, 1]
        
        print(f"   æ¸©åº¦ä¸SHAPå€¼ç›¸å…³æ€§: {temp_shap_corr:.3f}")
        
        # åˆ†ææç«¯æ¸©åº¦çš„å½±å“
        cold_mask = self.predict_data['temp'] < 0
        warm_mask = self.predict_data['temp'] > 10
        
        if np.sum(cold_mask) > 0:
            cold_shap = np.mean(self.shap_values[cold_mask, 0])
            print(f"   ä¸¥å¯’å¤©æ°”(<0Â°C)å¹³å‡SHAP: {cold_shap:.2f}")
        
        if np.sum(warm_mask) > 0:
            warm_shap = np.mean(self.shap_values[warm_mask, 0])
            print(f"   æ¸©æš–å¤©æ°”(>10Â°C)å¹³å‡SHAP: {warm_shap:.2f}")
        
        business_validation['temperature_logic'] = {
            'correlation': temp_shap_corr,
            'cold_effect': cold_shap if np.sum(cold_mask) > 0 else None,
            'warm_effect': warm_shap if np.sum(warm_mask) > 0 else None
        }
        
        # 3. æ˜ŸæœŸç‰¹å¾çš„åˆç†æ€§
        print("\n3. æ˜ŸæœŸç‰¹å¾åˆ†æ:")
        dow_effects = {}
        dow_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
        
        for dow in range(7):
            mask = self.predict_data['day_of_week'] == dow
            if np.sum(mask) > 0:
                avg_shap = np.mean(self.shap_values[mask, 2])  # Day_of_Weekæ˜¯ç¬¬3ä¸ªç‰¹å¾
                dow_effects[dow_names[dow]] = avg_shap
        
        weekday_avg = np.mean([dow_effects[day] for day in dow_names[:5] if day in dow_effects])
        weekend_avg = np.mean([dow_effects[day] for day in dow_names[5:] if day in dow_effects])
        
        print(f"   å·¥ä½œæ—¥å¹³å‡SHAP: {weekday_avg:.2f}")
        print(f"   å‘¨æœ«å¹³å‡SHAP: {weekend_avg:.2f}")
        print(f"   å·¥ä½œæ—¥vså‘¨æœ«å·®å¼‚: {weekday_avg - weekend_avg:.2f}")
        
        business_validation['weekday_logic'] = {
            'weekday_avg': weekday_avg,
            'weekend_avg': weekend_avg,
            'difference': weekday_avg - weekend_avg
        }
        
        return business_validation
        
    def generate_validation_report(self):
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        print("\nğŸ“‹ ç”ŸæˆSHAPéªŒè¯æŠ¥å‘Š...")
        
        # è¿è¡Œæ‰€æœ‰éªŒè¯
        model_performance = self.train_and_validate_model()
        self.initialize_and_calculate_shap()
        math_validation = self.validate_shap_properties()
        business_validation = self.validate_business_logic()
        
        # ç»¼åˆè¯„ä¼°
        print("\n" + "="*60)
        print("ğŸ¯ SHAPè®¡ç®—åˆç†æ€§ç»¼åˆè¯„ä¼°")
        print("="*60)
        
        print("\nâœ… æ•°å­¦æ€§è´¨éªŒè¯:")
        print(f"   æ•ˆç‡æ€§è´¨è¯¯å·®: {math_validation['efficiency_error']:.6f} (åº”æ¥è¿‘0)")
        print(f"   ä¸€è‡´æ€§ç›¸å…³æ€§: {math_validation['consistency_correlation']:.3f} (åº”>0.8)")
        
        print("\nâœ… ä¸šåŠ¡é€»è¾‘éªŒè¯:")
        print(f"   é«˜å³°æ—¶æ®µè¯†åˆ«: {business_validation['hour_logic']['peak_overlap']}/3 ç¬¦åˆé¢„æœŸ")
        print(f"   æ¸©åº¦å½±å“ç›¸å…³æ€§: {business_validation['temperature_logic']['correlation']:.3f}")
        print(f"   å·¥ä½œæ—¥vså‘¨æœ«å·®å¼‚: {business_validation['weekday_logic']['difference']:.2f}")
        
        print("\nâœ… æ¨¡å‹æ€§èƒ½:")
        print(f"   RÂ²å¾—åˆ†: {model_performance['r2']:.3f} (åº”>0.8)")
        print(f"   MAE: {model_performance['mae']:.2f} MW")
        
        # æœ€ç»ˆç»“è®º
        is_reasonable = (
            math_validation['efficiency_error'] < 1e-5 and
            math_validation['consistency_correlation'] > 0.8 and
            model_performance['r2'] > 0.8 and
            business_validation['hour_logic']['peak_overlap'] >= 1
        )
        
        print(f"\nğŸ‰ æœ€ç»ˆç»“è®º: SHAPè®¡ç®— {'åˆç†' if is_reasonable else 'éœ€è¦æ”¹è¿›'}")
        
        if is_reasonable:
            print("\nâœ… åˆç†æ€§åŸå› :")
            print("   1. æ»¡è¶³SHAPçš„æ•°å­¦æ€§è´¨ï¼ˆæ•ˆç‡æ€§ã€å¯¹ç§°æ€§ã€è™šæ‹Ÿæ€§ï¼‰")
            print("   2. ä¸XGBoostç‰¹å¾é‡è¦æ€§é«˜åº¦ä¸€è‡´")
            print("   3. ç¬¦åˆç”µåŠ›éœ€æ±‚çš„ä¸šåŠ¡é€»è¾‘")
            print("   4. æ¨¡å‹æ€§èƒ½è‰¯å¥½ï¼ŒåŸºç¡€å¯é ")
        
        return {
            'is_reasonable': is_reasonable,
            'model_performance': model_performance,
            'math_validation': math_validation,
            'business_validation': business_validation
        }
        
    def run_full_validation(self):
        """è¿è¡Œå®Œæ•´éªŒè¯"""
        print("ğŸš€ å¼€å§‹SHAPè®¡ç®—åˆç†æ€§éªŒè¯...")
        
        self.load_and_prepare_data()
        report = self.generate_validation_report()
        
        print("\nğŸ‰ éªŒè¯å®Œæˆï¼")
        return report

if __name__ == "__main__":
    validator = SHAPValidationAnalyzer()
    validation_report = validator.run_full_validation()
